{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('AllData/sensor_readings_24.csv', delimiter = ',', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('AllData/sensor_readings_24.csv', delimiter = ',', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bd17c50>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAFlCAYAAAAeWxREAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd3eb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Counts of each Action')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Action')\n",
    "df1[24].value_counts().plot.bar(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.645</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.351</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.648</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.637</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.629</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.637</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.437</td>\n",
       "      <td>0.501</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.626</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.353</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.626</td>\n",
       "      <td>3.629</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.640</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.439</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.626</td>\n",
       "      <td>3.629</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.633</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.430</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.440</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.627</td>\n",
       "      <td>3.628</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.919</td>\n",
       "      <td>3.028</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.330</td>\n",
       "      <td>2.638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.432</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.444</td>\n",
       "      <td>5.021</td>\n",
       "      <td>3.631</td>\n",
       "      <td>3.634</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.919</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.626</td>\n",
       "      <td>2.327</td>\n",
       "      <td>2.638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.436</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.451</td>\n",
       "      <td>5.025</td>\n",
       "      <td>3.635</td>\n",
       "      <td>3.639</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.920</td>\n",
       "      <td>3.027</td>\n",
       "      <td>2.620</td>\n",
       "      <td>2.323</td>\n",
       "      <td>2.632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.458</td>\n",
       "      <td>5.022</td>\n",
       "      <td>3.640</td>\n",
       "      <td>3.644</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.922</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.465</td>\n",
       "      <td>0.525</td>\n",
       "      <td>3.646</td>\n",
       "      <td>3.670</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.923</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.611</td>\n",
       "      <td>2.315</td>\n",
       "      <td>2.631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.473</td>\n",
       "      <td>0.533</td>\n",
       "      <td>3.652</td>\n",
       "      <td>3.676</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.925</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.607</td>\n",
       "      <td>2.310</td>\n",
       "      <td>2.623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.544</td>\n",
       "      <td>3.658</td>\n",
       "      <td>3.678</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.926</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.606</td>\n",
       "      <td>2.303</td>\n",
       "      <td>2.619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.484</td>\n",
       "      <td>0.544</td>\n",
       "      <td>3.661</td>\n",
       "      <td>3.665</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.928</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.304</td>\n",
       "      <td>5.022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.484</td>\n",
       "      <td>0.532</td>\n",
       "      <td>3.669</td>\n",
       "      <td>3.662</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.926</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.326</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.520</td>\n",
       "      <td>3.685</td>\n",
       "      <td>3.664</td>\n",
       "      <td>2.952</td>\n",
       "      <td>2.927</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.981</td>\n",
       "      <td>2.307</td>\n",
       "      <td>2.329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.522</td>\n",
       "      <td>3.682</td>\n",
       "      <td>3.661</td>\n",
       "      <td>2.955</td>\n",
       "      <td>2.927</td>\n",
       "      <td>2.957</td>\n",
       "      <td>2.984</td>\n",
       "      <td>1.700</td>\n",
       "      <td>2.622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.525</td>\n",
       "      <td>3.694</td>\n",
       "      <td>3.664</td>\n",
       "      <td>2.948</td>\n",
       "      <td>2.926</td>\n",
       "      <td>2.950</td>\n",
       "      <td>2.995</td>\n",
       "      <td>1.697</td>\n",
       "      <td>2.619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.515</td>\n",
       "      <td>5.018</td>\n",
       "      <td>3.664</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.927</td>\n",
       "      <td>2.947</td>\n",
       "      <td>2.993</td>\n",
       "      <td>1.697</td>\n",
       "      <td>2.622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.479</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.696</td>\n",
       "      <td>3.661</td>\n",
       "      <td>2.953</td>\n",
       "      <td>2.927</td>\n",
       "      <td>2.944</td>\n",
       "      <td>2.993</td>\n",
       "      <td>1.702</td>\n",
       "      <td>2.622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.506</td>\n",
       "      <td>5.019</td>\n",
       "      <td>3.665</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.929</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.981</td>\n",
       "      <td>1.700</td>\n",
       "      <td>2.616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.479</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.019</td>\n",
       "      <td>3.661</td>\n",
       "      <td>2.943</td>\n",
       "      <td>2.930</td>\n",
       "      <td>2.942</td>\n",
       "      <td>2.996</td>\n",
       "      <td>1.698</td>\n",
       "      <td>2.312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.479</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.020</td>\n",
       "      <td>3.662</td>\n",
       "      <td>2.945</td>\n",
       "      <td>2.931</td>\n",
       "      <td>2.942</td>\n",
       "      <td>2.997</td>\n",
       "      <td>1.700</td>\n",
       "      <td>2.313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.473</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.508</td>\n",
       "      <td>5.021</td>\n",
       "      <td>3.660</td>\n",
       "      <td>2.954</td>\n",
       "      <td>2.936</td>\n",
       "      <td>2.946</td>\n",
       "      <td>2.966</td>\n",
       "      <td>1.705</td>\n",
       "      <td>2.313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.473</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.486</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.522</td>\n",
       "      <td>3.662</td>\n",
       "      <td>2.958</td>\n",
       "      <td>2.938</td>\n",
       "      <td>2.939</td>\n",
       "      <td>2.627</td>\n",
       "      <td>1.707</td>\n",
       "      <td>2.314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.474</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.479</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.520</td>\n",
       "      <td>3.663</td>\n",
       "      <td>2.954</td>\n",
       "      <td>2.938</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.957</td>\n",
       "      <td>1.712</td>\n",
       "      <td>2.314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.474</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.521</td>\n",
       "      <td>3.664</td>\n",
       "      <td>2.954</td>\n",
       "      <td>2.938</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.632</td>\n",
       "      <td>1.715</td>\n",
       "      <td>2.313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.476</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.481</td>\n",
       "      <td>0.511</td>\n",
       "      <td>5.023</td>\n",
       "      <td>3.665</td>\n",
       "      <td>2.954</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.627</td>\n",
       "      <td>1.707</td>\n",
       "      <td>2.312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.477</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.524</td>\n",
       "      <td>3.665</td>\n",
       "      <td>2.953</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.940</td>\n",
       "      <td>2.629</td>\n",
       "      <td>1.706</td>\n",
       "      <td>2.312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.477</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.482</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.521</td>\n",
       "      <td>3.665</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.940</td>\n",
       "      <td>2.940</td>\n",
       "      <td>2.627</td>\n",
       "      <td>1.706</td>\n",
       "      <td>2.312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.477</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>4.410</td>\n",
       "      <td>3.626</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.643</td>\n",
       "      <td>2.650</td>\n",
       "      <td>2.594</td>\n",
       "      <td>1.411</td>\n",
       "      <td>4.860</td>\n",
       "      <td>1.361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.878</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.717</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>4.414</td>\n",
       "      <td>3.651</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.664</td>\n",
       "      <td>2.645</td>\n",
       "      <td>2.654</td>\n",
       "      <td>2.521</td>\n",
       "      <td>1.411</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.868</td>\n",
       "      <td>2.347</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>4.411</td>\n",
       "      <td>3.629</td>\n",
       "      <td>3.686</td>\n",
       "      <td>2.666</td>\n",
       "      <td>2.647</td>\n",
       "      <td>2.659</td>\n",
       "      <td>2.523</td>\n",
       "      <td>1.410</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.923</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.847</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>4.416</td>\n",
       "      <td>3.630</td>\n",
       "      <td>3.687</td>\n",
       "      <td>2.667</td>\n",
       "      <td>2.648</td>\n",
       "      <td>2.617</td>\n",
       "      <td>2.601</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.782</td>\n",
       "      <td>2.119</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.895</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>4.415</td>\n",
       "      <td>3.633</td>\n",
       "      <td>3.689</td>\n",
       "      <td>2.665</td>\n",
       "      <td>2.649</td>\n",
       "      <td>2.657</td>\n",
       "      <td>2.519</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.384</td>\n",
       "      <td>1.360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.782</td>\n",
       "      <td>2.120</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.922</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>4.422</td>\n",
       "      <td>3.638</td>\n",
       "      <td>3.699</td>\n",
       "      <td>2.663</td>\n",
       "      <td>2.650</td>\n",
       "      <td>2.614</td>\n",
       "      <td>2.514</td>\n",
       "      <td>1.402</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.911</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5432</th>\n",
       "      <td>4.421</td>\n",
       "      <td>3.652</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.672</td>\n",
       "      <td>2.653</td>\n",
       "      <td>2.609</td>\n",
       "      <td>2.515</td>\n",
       "      <td>1.397</td>\n",
       "      <td>1.369</td>\n",
       "      <td>1.346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.880</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5433</th>\n",
       "      <td>4.427</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.673</td>\n",
       "      <td>2.657</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.505</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1.359</td>\n",
       "      <td>1.335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.776</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.882</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5434</th>\n",
       "      <td>4.438</td>\n",
       "      <td>3.674</td>\n",
       "      <td>3.736</td>\n",
       "      <td>2.669</td>\n",
       "      <td>2.662</td>\n",
       "      <td>2.608</td>\n",
       "      <td>2.502</td>\n",
       "      <td>1.374</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.768</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.889</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.754</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5435</th>\n",
       "      <td>4.444</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.749</td>\n",
       "      <td>2.678</td>\n",
       "      <td>2.667</td>\n",
       "      <td>2.606</td>\n",
       "      <td>2.502</td>\n",
       "      <td>1.360</td>\n",
       "      <td>1.649</td>\n",
       "      <td>1.309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.899</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5436</th>\n",
       "      <td>4.454</td>\n",
       "      <td>3.705</td>\n",
       "      <td>3.741</td>\n",
       "      <td>2.683</td>\n",
       "      <td>2.673</td>\n",
       "      <td>2.717</td>\n",
       "      <td>2.497</td>\n",
       "      <td>1.341</td>\n",
       "      <td>1.317</td>\n",
       "      <td>1.293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.759</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.903</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.799</td>\n",
       "      <td>2.005</td>\n",
       "      <td>1.624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437</th>\n",
       "      <td>4.464</td>\n",
       "      <td>3.721</td>\n",
       "      <td>2.718</td>\n",
       "      <td>2.689</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.697</td>\n",
       "      <td>2.494</td>\n",
       "      <td>1.324</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1.276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.753</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.912</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.036</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5438</th>\n",
       "      <td>4.474</td>\n",
       "      <td>3.730</td>\n",
       "      <td>2.722</td>\n",
       "      <td>2.697</td>\n",
       "      <td>2.686</td>\n",
       "      <td>3.554</td>\n",
       "      <td>2.497</td>\n",
       "      <td>1.309</td>\n",
       "      <td>1.285</td>\n",
       "      <td>1.264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.973</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.073</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>4.485</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.733</td>\n",
       "      <td>2.704</td>\n",
       "      <td>2.693</td>\n",
       "      <td>2.704</td>\n",
       "      <td>2.487</td>\n",
       "      <td>1.294</td>\n",
       "      <td>1.272</td>\n",
       "      <td>1.248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.977</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.102</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>4.495</td>\n",
       "      <td>2.771</td>\n",
       "      <td>3.808</td>\n",
       "      <td>2.710</td>\n",
       "      <td>2.699</td>\n",
       "      <td>2.708</td>\n",
       "      <td>2.487</td>\n",
       "      <td>1.278</td>\n",
       "      <td>1.603</td>\n",
       "      <td>1.231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.732</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.978</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.138</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>4.506</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.746</td>\n",
       "      <td>2.716</td>\n",
       "      <td>2.708</td>\n",
       "      <td>2.741</td>\n",
       "      <td>2.473</td>\n",
       "      <td>1.268</td>\n",
       "      <td>1.566</td>\n",
       "      <td>1.215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.725</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.987</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.178</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>1.515</td>\n",
       "      <td>3.803</td>\n",
       "      <td>2.755</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.711</td>\n",
       "      <td>2.701</td>\n",
       "      <td>2.467</td>\n",
       "      <td>1.248</td>\n",
       "      <td>1.226</td>\n",
       "      <td>1.199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.718</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.995</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5443</th>\n",
       "      <td>4.540</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.761</td>\n",
       "      <td>2.731</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.689</td>\n",
       "      <td>2.463</td>\n",
       "      <td>1.232</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.712</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>4.547</td>\n",
       "      <td>3.834</td>\n",
       "      <td>3.870</td>\n",
       "      <td>2.734</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.460</td>\n",
       "      <td>1.215</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.706</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.998</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5445</th>\n",
       "      <td>4.556</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.890</td>\n",
       "      <td>2.743</td>\n",
       "      <td>2.731</td>\n",
       "      <td>2.323</td>\n",
       "      <td>2.456</td>\n",
       "      <td>1.199</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.699</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.017</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5446</th>\n",
       "      <td>1.316</td>\n",
       "      <td>3.873</td>\n",
       "      <td>3.907</td>\n",
       "      <td>2.749</td>\n",
       "      <td>2.738</td>\n",
       "      <td>2.326</td>\n",
       "      <td>2.449</td>\n",
       "      <td>1.177</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.692</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.010</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>4.586</td>\n",
       "      <td>3.884</td>\n",
       "      <td>3.923</td>\n",
       "      <td>2.755</td>\n",
       "      <td>2.743</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.445</td>\n",
       "      <td>1.163</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.757</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.010</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>4.604</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.944</td>\n",
       "      <td>2.765</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.688</td>\n",
       "      <td>2.445</td>\n",
       "      <td>1.142</td>\n",
       "      <td>1.129</td>\n",
       "      <td>1.095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.750</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.020</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>1.646</td>\n",
       "      <td>3.925</td>\n",
       "      <td>3.958</td>\n",
       "      <td>2.771</td>\n",
       "      <td>2.756</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.439</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.785</td>\n",
       "      <td>1.077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.753</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.026</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>0.911</td>\n",
       "      <td>3.939</td>\n",
       "      <td>3.976</td>\n",
       "      <td>2.775</td>\n",
       "      <td>2.763</td>\n",
       "      <td>2.573</td>\n",
       "      <td>2.436</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.781</td>\n",
       "      <td>1.059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1.591</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.035</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>0.910</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.997</td>\n",
       "      <td>2.785</td>\n",
       "      <td>2.770</td>\n",
       "      <td>2.572</td>\n",
       "      <td>2.433</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.772</td>\n",
       "      <td>1.040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.686</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.045</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5452</th>\n",
       "      <td>0.926</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.015</td>\n",
       "      <td>2.792</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.571</td>\n",
       "      <td>1.768</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.762</td>\n",
       "      <td>1.021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.593</td>\n",
       "      <td>1.616</td>\n",
       "      <td>1.058</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5453</th>\n",
       "      <td>0.937</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.034</td>\n",
       "      <td>2.799</td>\n",
       "      <td>2.784</td>\n",
       "      <td>2.571</td>\n",
       "      <td>1.754</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.752</td>\n",
       "      <td>1.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.741</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.065</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>0.945</td>\n",
       "      <td>4.052</td>\n",
       "      <td>4.052</td>\n",
       "      <td>2.809</td>\n",
       "      <td>2.791</td>\n",
       "      <td>2.441</td>\n",
       "      <td>1.757</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.743</td>\n",
       "      <td>0.983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.754</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.076</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>0.950</td>\n",
       "      <td>4.066</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.819</td>\n",
       "      <td>2.798</td>\n",
       "      <td>2.570</td>\n",
       "      <td>2.422</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.739</td>\n",
       "      <td>0.964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.776</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.083</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5456 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9   \\\n",
       "0     0.438  0.498  3.625  3.645  5.000  2.918  5.000  2.351  2.332  2.643   \n",
       "1     0.438  0.498  3.625  3.648  5.000  2.918  5.000  2.637  2.332  2.649   \n",
       "2     0.438  0.498  3.625  3.629  5.000  2.918  5.000  2.637  2.334  2.643   \n",
       "3     0.437  0.501  3.625  3.626  5.000  2.918  5.000  2.353  2.334  2.642   \n",
       "4     0.438  0.498  3.626  3.629  5.000  2.918  5.000  2.640  2.334  2.639   \n",
       "5     0.439  0.498  3.626  3.629  5.000  2.918  5.000  2.633  2.334  2.645   \n",
       "6     0.440  5.000  3.627  3.628  5.000  2.919  3.028  2.346  2.330  2.638   \n",
       "7     0.444  5.021  3.631  3.634  5.000  2.919  5.000  2.626  2.327  2.638   \n",
       "8     0.451  5.025  3.635  3.639  5.000  2.920  3.027  2.620  2.323  2.632   \n",
       "9     0.458  5.022  3.640  3.644  5.000  2.922  5.000  2.346  2.321  2.628   \n",
       "10    0.465  0.525  3.646  3.670  5.000  2.923  5.000  2.611  2.315  2.631   \n",
       "11    0.473  0.533  3.652  3.676  5.000  2.925  5.000  2.607  2.310  2.623   \n",
       "12    0.481  0.544  3.658  3.678  5.000  2.926  5.000  2.606  2.303  2.619   \n",
       "13    0.484  0.544  3.661  3.665  5.000  2.928  5.000  2.321  2.304  5.022   \n",
       "14    0.484  0.532  3.669  3.662  2.945  2.926  5.000  2.326  2.306  2.620   \n",
       "15    0.482  0.520  3.685  3.664  2.952  2.927  5.000  2.981  2.307  2.329   \n",
       "16    0.481  0.522  3.682  3.661  2.955  2.927  2.957  2.984  1.700  2.622   \n",
       "17    0.480  0.525  3.694  3.664  2.948  2.926  2.950  2.995  1.697  2.619   \n",
       "18    0.481  0.515  5.018  3.664  2.956  2.927  2.947  2.993  1.697  2.622   \n",
       "19    0.479  5.000  3.696  3.661  2.953  2.927  2.944  2.993  1.702  2.622   \n",
       "20    0.480  0.506  5.019  3.665  2.941  2.929  2.945  2.981  1.700  2.616   \n",
       "21    0.479  5.000  5.019  3.661  2.943  2.930  2.942  2.996  1.698  2.312   \n",
       "22    0.479  5.000  5.020  3.662  2.945  2.931  2.942  2.997  1.700  2.313   \n",
       "23    0.481  0.508  5.021  3.660  2.954  2.936  2.946  2.966  1.705  2.313   \n",
       "24    0.486  0.510  0.522  3.662  2.958  2.938  2.939  2.627  1.707  2.314   \n",
       "25    0.479  0.521  0.520  3.663  2.954  2.938  2.941  2.957  1.712  2.314   \n",
       "26    0.480  0.554  0.521  3.664  2.954  2.938  2.941  2.632  1.715  2.313   \n",
       "27    0.481  0.511  5.023  3.665  2.954  2.937  2.941  2.627  1.707  2.312   \n",
       "28    0.482  0.528  0.524  3.665  2.953  2.937  2.940  2.629  1.706  2.312   \n",
       "29    0.482  0.529  0.521  3.665  2.956  2.940  2.940  2.627  1.706  2.312   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5426  4.410  3.626  5.000  5.000  2.643  2.650  2.594  1.411  4.860  1.361   \n",
       "5427  4.414  3.651  5.000  2.664  2.645  2.654  2.521  1.411  1.389  1.363   \n",
       "5428  4.411  3.629  3.686  2.666  2.647  2.659  2.523  1.410  1.392  1.364   \n",
       "5429  4.416  3.630  3.687  2.667  2.648  2.617  2.601  1.409  1.391  1.364   \n",
       "5430  4.415  3.633  3.689  2.665  2.649  2.657  2.519  1.409  1.384  1.360   \n",
       "5431  4.422  3.638  3.699  2.663  2.650  2.614  2.514  1.402  1.378  1.354   \n",
       "5432  4.421  3.652  5.000  2.672  2.653  2.609  2.515  1.397  1.369  1.346   \n",
       "5433  4.427  5.000  5.000  2.673  2.657  2.604  2.505  1.386  1.359  1.335   \n",
       "5434  4.438  3.674  3.736  2.669  2.662  2.608  2.502  1.374  1.350  1.323   \n",
       "5435  4.444  5.000  3.749  2.678  2.667  2.606  2.502  1.360  1.649  1.309   \n",
       "5436  4.454  3.705  3.741  2.683  2.673  2.717  2.497  1.341  1.317  1.293   \n",
       "5437  4.464  3.721  2.718  2.689  2.680  2.697  2.494  1.324  1.300  1.276   \n",
       "5438  4.474  3.730  2.722  2.697  2.686  3.554  2.497  1.309  1.285  1.264   \n",
       "5439  4.485  5.000  2.733  2.704  2.693  2.704  2.487  1.294  1.272  1.248   \n",
       "5440  4.495  2.771  3.808  2.710  2.699  2.708  2.487  1.278  1.603  1.231   \n",
       "5441  4.506  5.000  2.746  2.716  2.708  2.741  2.473  1.268  1.566  1.215   \n",
       "5442  1.515  3.803  2.755  2.725  2.711  2.701  2.467  1.248  1.226  1.199   \n",
       "5443  4.540  5.000  2.761  2.731  2.720  2.689  2.463  1.232  5.000  1.180   \n",
       "5444  4.547  3.834  3.870  2.734  2.725  2.328  2.460  1.215  5.000  1.166   \n",
       "5445  4.556  5.000  3.890  2.743  2.731  2.323  2.456  1.199  5.000  1.149   \n",
       "5446  1.316  3.873  3.907  2.749  2.738  2.326  2.449  1.177  5.000  1.128   \n",
       "5447  4.586  3.884  3.923  2.755  2.743  2.334  2.445  1.163  5.000  1.113   \n",
       "5448  4.604  5.000  3.944  2.765  2.750  2.688  2.445  1.142  1.129  1.095   \n",
       "5449  1.646  3.925  3.958  2.771  2.756  2.337  2.439  1.127  1.785  1.077   \n",
       "5450  0.911  3.939  3.976  2.775  2.763  2.573  2.436  1.106  1.781  1.059   \n",
       "5451  0.910  5.000  3.997  2.785  2.770  2.572  2.433  1.087  1.772  1.040   \n",
       "5452  0.926  5.000  4.015  2.792  2.777  2.571  1.768  1.071  1.762  1.021   \n",
       "5453  0.937  5.000  4.034  2.799  2.784  2.571  1.754  1.053  1.752  1.002   \n",
       "5454  0.945  4.052  4.052  2.809  2.791  2.441  1.757  1.034  1.743  0.983   \n",
       "5455  0.950  4.066  5.000  2.819  2.798  2.570  2.422  1.016  1.739  0.964   \n",
       "\n",
       "      ...     15     16     17     18     19     20     21     22     23  24  \n",
       "0     ...  0.593  0.502  0.493  0.504  0.445  0.431  0.444  0.440  0.429   3  \n",
       "1     ...  0.592  0.502  0.493  0.504  0.449  0.431  0.444  0.443  0.429   3  \n",
       "2     ...  0.593  0.502  0.493  0.504  0.449  0.431  0.444  0.446  0.429   3  \n",
       "3     ...  0.593  0.502  0.493  0.504  0.449  0.431  0.444  0.444  0.429   3  \n",
       "4     ...  0.592  0.502  0.493  0.504  0.449  0.431  0.444  0.441  0.429   3  \n",
       "5     ...  0.589  0.502  0.493  0.504  0.446  0.431  0.444  0.444  0.430   3  \n",
       "6     ...  0.588  0.501  0.492  0.504  0.451  0.433  0.446  0.444  0.432   3  \n",
       "7     ...  0.595  0.500  0.491  0.503  0.453  0.436  0.448  0.444  0.436   3  \n",
       "8     ...  0.595  0.499  0.491  0.502  0.457  0.440  0.453  0.454  0.442   1  \n",
       "9     ...  0.590  0.496  0.490  0.498  0.462  0.444  0.458  0.461  0.449   1  \n",
       "10    ...  0.593  0.495  0.488  0.497  0.467  0.449  0.462  0.469  0.457   1  \n",
       "11    ...  0.578  0.496  0.487  0.498  0.469  0.454  0.467  0.476  0.465   1  \n",
       "12    ...  0.581  0.495  0.486  0.497  0.477  0.459  0.472  0.484  0.472   1  \n",
       "13    ...  0.623  0.493  0.484  0.495  0.480  0.461  0.474  0.485  0.476   1  \n",
       "14    ...  0.533  0.493  0.483  0.494  0.507  0.461  0.473  0.486  0.476   1  \n",
       "15    ...  0.533  0.492  0.482  0.492  0.513  0.459  0.474  0.485  0.474   1  \n",
       "16    ...  0.530  0.492  0.482  0.492  0.513  0.462  0.486  0.483  0.473   1  \n",
       "17    ...  0.530  0.493  0.482  0.492  0.516  0.462  0.486  0.483  0.473   1  \n",
       "18    ...  0.592  0.489  0.482  0.495  0.531  0.462  0.499  0.483  0.473   1  \n",
       "19    ...  0.588  0.489  0.481  0.491  0.510  0.462  0.481  0.483  0.473   1  \n",
       "20    ...  0.643  0.491  0.480  0.493  0.524  0.461  0.469  0.484  0.473   1  \n",
       "21    ...  0.617  0.491  0.479  0.491  0.575  0.461  0.465  0.484  0.473   1  \n",
       "22    ...  0.616  0.490  0.478  0.489  0.503  0.460  0.460  0.478  0.473   3  \n",
       "23    ...  0.678  0.493  0.477  0.483  0.497  0.467  0.459  0.476  0.473   3  \n",
       "24    ...  0.696  0.516  0.476  0.477  0.547  0.465  0.457  0.470  0.474   3  \n",
       "25    ...  0.688  0.534  0.475  0.475  0.489  0.461  0.456  0.465  0.474   3  \n",
       "26    ...  0.687  0.553  0.475  0.474  0.558  0.462  0.453  0.465  0.476   3  \n",
       "27    ...  0.687  0.545  0.475  0.475  0.504  0.463  0.458  0.470  0.477   3  \n",
       "28    ...  0.566  0.549  0.475  0.476  0.505  0.464  0.459  0.468  0.477   3  \n",
       "29    ...  0.566  0.556  0.475  0.476  0.502  0.463  0.459  0.468  0.477   3  \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ..  \n",
       "5426  ...  0.790  0.780  0.792  0.816  0.869  0.878  5.000  0.717  5.000   0  \n",
       "5427  ...  0.789  0.778  0.789  0.816  0.868  2.347  5.000  0.750  0.716   0  \n",
       "5428  ...  0.788  0.777  0.783  0.809  0.827  0.923  5.000  1.847  0.708   0  \n",
       "5429  ...  0.787  0.776  0.782  2.119  0.870  0.895  5.000  5.000  0.706   0  \n",
       "5430  ...  0.786  0.775  0.782  2.120  0.872  0.922  5.000  0.758  0.712   0  \n",
       "5431  ...  0.784  0.774  0.781  0.895  0.876  0.911  5.000  0.753  0.722   0  \n",
       "5432  ...  0.781  0.771  0.781  0.804  0.880  5.000  5.000  1.889  0.727   0  \n",
       "5433  ...  0.777  0.766  0.776  5.000  0.882  5.000  5.000  0.775  0.748   0  \n",
       "5434  ...  0.772  0.761  0.768  5.000  0.889  5.000  5.000  0.797  0.754   1  \n",
       "5435  ...  0.766  0.755  0.766  0.789  0.899  5.000  5.000  0.797  0.775   0  \n",
       "5436  ...  0.760  0.749  0.759  5.000  0.903  5.000  2.799  2.005  1.624   0  \n",
       "5437  ...  0.754  0.743  0.753  5.000  0.912  5.000  5.000  2.036  0.801   0  \n",
       "5438  ...  0.746  0.736  0.745  0.772  0.920  0.973  5.000  2.073  0.817   0  \n",
       "5439  ...  0.740  0.729  0.738  0.761  0.931  0.977  5.000  2.102  0.864   0  \n",
       "5440  ...  0.733  0.722  0.732  5.000  0.974  0.978  5.000  2.138  0.851   0  \n",
       "5441  ...  0.727  0.716  0.725  5.000  0.775  0.987  5.000  2.178  0.860   0  \n",
       "5442  ...  0.720  0.709  0.718  5.000  5.000  0.995  5.000  5.000  0.878   0  \n",
       "5443  ...  0.714  0.703  0.712  5.000  1.000  1.000  5.000  5.000  0.898   0  \n",
       "5444  ...  0.708  0.696  0.706  5.000  5.000  0.998  5.000  5.000  0.919   0  \n",
       "5445  ...  0.701  0.690  0.699  5.000  5.000  1.017  5.000  5.000  1.576   0  \n",
       "5446  ...  0.695  0.683  0.692  5.000  5.000  1.010  5.000  5.000  0.966   0  \n",
       "5447  ...  0.688  0.676  0.686  0.757  5.000  1.010  5.000  5.000  5.000   0  \n",
       "5448  ...  0.681  0.669  0.678  0.750  5.000  1.020  5.000  5.000  1.007   0  \n",
       "5449  ...  0.674  0.662  0.672  0.753  5.000  1.026  5.000  5.000  1.594   0  \n",
       "5450  ...  0.667  0.655  0.665  1.591  5.000  1.035  5.000  5.000  5.000   0  \n",
       "5451  ...  0.660  0.648  0.657  0.686  5.000  1.045  5.000  5.000  1.562   0  \n",
       "5452  ...  0.652  0.640  0.649  1.593  1.616  1.058  5.000  5.000  1.085   1  \n",
       "5453  ...  0.648  0.633  0.642  0.741  5.000  1.065  5.000  5.000  1.105   1  \n",
       "5454  ...  0.641  0.626  0.635  0.754  5.000  1.076  5.000  5.000  1.118   0  \n",
       "5455  ...  0.635  0.618  0.628  0.776  5.000  1.083  5.000  5.000  1.168   1  \n",
       "\n",
       "[5456 rows x 25 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.00000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "      <td>5456.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.471617</td>\n",
       "      <td>2.327043</td>\n",
       "      <td>2.489347</td>\n",
       "      <td>2.796501</td>\n",
       "      <td>2.958552</td>\n",
       "      <td>2.893073</td>\n",
       "      <td>3.351113</td>\n",
       "      <td>2.540397</td>\n",
       "      <td>3.125621</td>\n",
       "      <td>2.832386</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202111</td>\n",
       "      <td>0.989831</td>\n",
       "      <td>0.910273</td>\n",
       "      <td>1.05811</td>\n",
       "      <td>1.076320</td>\n",
       "      <td>1.015923</td>\n",
       "      <td>1.778034</td>\n",
       "      <td>1.555045</td>\n",
       "      <td>1.578508</td>\n",
       "      <td>0.958761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.802801</td>\n",
       "      <td>1.410146</td>\n",
       "      <td>1.247435</td>\n",
       "      <td>1.309368</td>\n",
       "      <td>1.339225</td>\n",
       "      <td>1.282575</td>\n",
       "      <td>1.413692</td>\n",
       "      <td>1.111554</td>\n",
       "      <td>1.356965</td>\n",
       "      <td>1.307843</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098568</td>\n",
       "      <td>0.942075</td>\n",
       "      <td>0.889527</td>\n",
       "      <td>1.14463</td>\n",
       "      <td>1.141498</td>\n",
       "      <td>0.887439</td>\n",
       "      <td>1.571686</td>\n",
       "      <td>1.291447</td>\n",
       "      <td>1.150480</td>\n",
       "      <td>1.033599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.114000</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.34000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.921000</td>\n",
       "      <td>1.362000</td>\n",
       "      <td>1.538750</td>\n",
       "      <td>1.731000</td>\n",
       "      <td>1.774000</td>\n",
       "      <td>1.785750</td>\n",
       "      <td>1.930750</td>\n",
       "      <td>1.618000</td>\n",
       "      <td>1.799750</td>\n",
       "      <td>1.636000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.529750</td>\n",
       "      <td>0.52300</td>\n",
       "      <td>0.541750</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.335000</td>\n",
       "      <td>1.904500</td>\n",
       "      <td>2.064000</td>\n",
       "      <td>2.458000</td>\n",
       "      <td>2.667000</td>\n",
       "      <td>2.682500</td>\n",
       "      <td>3.225500</td>\n",
       "      <td>2.172000</td>\n",
       "      <td>2.802000</td>\n",
       "      <td>2.679000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.69100</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>1.030500</td>\n",
       "      <td>1.071000</td>\n",
       "      <td>1.289000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.814000</td>\n",
       "      <td>2.681500</td>\n",
       "      <td>2.739250</td>\n",
       "      <td>4.093500</td>\n",
       "      <td>4.314500</td>\n",
       "      <td>3.835250</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.193000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.526250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159000</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.85700</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>1.002250</td>\n",
       "      <td>2.068250</td>\n",
       "      <td>1.559500</td>\n",
       "      <td>1.657250</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.025000</td>\n",
       "      <td>5.029000</td>\n",
       "      <td>5.017000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.005000</td>\n",
       "      <td>5.008000</td>\n",
       "      <td>5.087000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.022000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  5456.000000  5456.000000  5456.000000  5456.000000  5456.000000   \n",
       "mean      1.471617     2.327043     2.489347     2.796501     2.958552   \n",
       "std       0.802801     1.410146     1.247435     1.309368     1.339225   \n",
       "min       0.400000     0.437000     0.470000     0.833000     1.120000   \n",
       "25%       0.921000     1.362000     1.538750     1.731000     1.774000   \n",
       "50%       1.335000     1.904500     2.064000     2.458000     2.667000   \n",
       "75%       1.814000     2.681500     2.739250     4.093500     4.314500   \n",
       "max       5.000000     5.025000     5.029000     5.017000     5.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  5456.000000  5456.000000  5456.000000  5456.000000  5456.000000  ...   \n",
       "mean      2.893073     3.351113     2.540397     3.125621     2.832386  ...   \n",
       "std       1.282575     1.413692     1.111554     1.356965     1.307843  ...   \n",
       "min       1.114000     1.122000     0.859000     0.836000     0.810000  ...   \n",
       "25%       1.785750     1.930750     1.618000     1.799750     1.636000  ...   \n",
       "50%       2.682500     3.225500     2.172000     2.802000     2.679000  ...   \n",
       "75%       3.835250     5.000000     3.193000     5.000000     3.526250  ...   \n",
       "max       5.005000     5.008000     5.087000     5.000000     5.022000  ...   \n",
       "\n",
       "                15           16           17          18           19  \\\n",
       "count  5456.000000  5456.000000  5456.000000  5456.00000  5456.000000   \n",
       "mean      1.202111     0.989831     0.910273     1.05811     1.076320   \n",
       "std       1.098568     0.942075     0.889527     1.14463     1.141498   \n",
       "min       0.424000     0.373000     0.354000     0.34000     0.355000   \n",
       "25%       0.690000     0.581000     0.529750     0.52300     0.541750   \n",
       "50%       0.803000     0.738000     0.685000     0.69100     0.693000   \n",
       "75%       1.159000     0.913000     0.837000     0.85700     0.863000   \n",
       "max       5.000000     5.000000     5.000000     5.00000     5.000000   \n",
       "\n",
       "                20           21           22           23           24  \n",
       "count  5456.000000  5456.000000  5456.000000  5456.000000  5456.000000  \n",
       "mean      1.015923     1.778034     1.555045     1.578508     0.958761  \n",
       "std       0.887439     1.571686     1.291447     1.150480     1.033599  \n",
       "min       0.380000     0.370000     0.367000     0.377000     0.000000  \n",
       "25%       0.567000     0.743000     0.792000     0.884000     0.000000  \n",
       "50%       0.764000     1.030500     1.071000     1.289000     1.000000  \n",
       "75%       1.002250     2.068250     1.559500     1.657250     1.000000  \n",
       "max       5.000000     5.000000     5.000000     5.000000     3.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "df[24] = labelencoder.fit_transform(df[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Move-Forward', 'Sharp-Right-Turn', 'Slight-Right-Turn',\n",
       "       'Slight-Left-Turn'], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.inverse_transform(value_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.iloc[:,:-2].values\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4092, 23)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1364, 23)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.6730205278592375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       543\n",
      "           1       0.79      0.65      0.72       628\n",
      "           2       0.39      0.56      0.46        55\n",
      "           3       0.43      0.64      0.51       138\n",
      "\n",
      "    accuracy                           0.67      1364\n",
      "   macro avg       0.58      0.64      0.60      1364\n",
      "weighted avg       0.70      0.67      0.68      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = log_reg.score(x_test, y_test)\n",
    "print('Score: ', score)\n",
    "print(classification_report(log_reg.predict(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest=RandomForestClassifier()\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "forest.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "n_estimators = [10, 50, 100, 300]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "max_features  = [5, 10, 15, 20, 23]\n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              max_features = max_features)\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "bestF = gridF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 25, 'max_features': 23, 'n_estimators': 100}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rating:  0.9970674486803519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       549\n",
      "           1       1.00      1.00      1.00       529\n",
      "           2       1.00      1.00      1.00        81\n",
      "           3       1.00      1.00      1.00       205\n",
      "\n",
      "    accuracy                           1.00      1364\n",
      "   macro avg       1.00      1.00      1.00      1364\n",
      "weighted avg       1.00      1.00      1.00      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_forest_estimator = gridF.best_estimator_\n",
    "print(\"Accuracy Rating: \", best_forest_estimator.score(x_test, y_test))\n",
    "print(classification_report(best_forest_estimator.predict(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def plot_feature(feature, feature_range):\n",
    "    feature_data = []\n",
    "    for r in feature_range:\n",
    "        forest = RandomForestClassifier(**{feature: r, 'n_estimators': 100, 'max_features': 15})\n",
    "        forest.fit(x_train, y_train)\n",
    "        feature_data.append(f1_score(y_test, forest.predict(x_test), average='weighted'))\n",
    "    plt.scatter(feature_range, feature_data)\n",
    "    plt.title('Effect of Changing %s on F1-Score' % (feature))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb18a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature('max_depth', [5, 8, 15, 25, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=1,\n",
       "    decision_function_shape='ovr', degree=4, gamma='auto_deprecated',\n",
       "    kernel='poly', max_iter=-1, probability=False, random_state=0,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'poly', random_state = 0, degree = 4, coef0 = 1)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9GZmmXPQm5llzkFvZpa5/w9FDPeYmmxyaAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c7e5030>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_svm(feature, feature_range):\n",
    "    feature_data = []\n",
    "    for r in feature_range:\n",
    "        svm_inner = SVC(**{feature: r, 'kernel': 'poly'})\n",
    "        svm_inner.fit(x_train, y_train)\n",
    "        feature_data.append(f1_score(y_test, svm_inner.predict(x_test), average='weighted'))  \n",
    "    if type(feature_range[0]) == str:\n",
    "        plt.scatter(np.arange(0, len(feature_range)), feature_data)\n",
    "        plt.xticks(np.arange(0, len(feature_range)), feature_range)\n",
    "    else:\n",
    "        plt.scatter(feature_range, feature_data)\n",
    "    plt.title('Effect of Changing %s on F1-Score' % (feature))\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_svm('degree', [2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "kernel = ['rbf', 'sigmoid', 'poly']\n",
    "gamma = [0.1, 0.25, 0.6, 0.8]\n",
    "C  = [1, 2, 3, 4]\n",
    "\n",
    "hyperF_svm = {'kernel': kernel, 'gamma': gamma, 'C': C}\n",
    "\n",
    "svm = SVC()\n",
    "gridF_svm = GridSearchCV(svm, hyperF_svm, cv = 3, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "bestF_svm = gridF_svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'gamma': 0.1, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridF_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rating:  0.9303519061583577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       579\n",
      "           1       0.93      0.97      0.95       505\n",
      "           2       0.91      0.91      0.91        81\n",
      "           3       0.89      0.92      0.91       199\n",
      "\n",
      "    accuracy                           0.93      1364\n",
      "   macro avg       0.92      0.93      0.92      1364\n",
      "weighted avg       0.93      0.93      0.93      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_svm_estimator = gridF_svm.best_estimator_\n",
    "print(\"Accuracy Rating: \", best_svm_estimator.score(x_test, y_test))\n",
    "print(classification_report(best_svm_estimator.predict(x_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# From: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(num_hidden_layers = 1, num_hidden_nodes = 5, activation = 'relu', batch_size = 5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_hidden_nodes, activation = activation, input_dim = 23))\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_hidden_nodes, kernel_initializer = 'uniform', activation = activation))\n",
    "    model.add(Dense(4, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = [f1_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation = 'relu', input_dim = 23))\n",
    "#model.add(Dense(10, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "#model.add(Dense(20, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "model.add(Dense(4, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = [f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-e47ede3023df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#plot_NN_models('num_hidden_layers', [1,2,3,4,5])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#plot_NN_models('activation', ['relu', 'tanh', 'sigmoid'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mplot_NN_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-309-e47ede3023df>\u001b[0m in \u001b[0;36mplot_NN_models\u001b[1;34m(feature, feature_range)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'num_hidden_nodes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_binary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'num_hidden_nodes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sachin konan\\documents\\python\\lib\\site-packages\\theano\\tensor\\blas.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out)\u001b[0m\n\u001b[0;32m   1825\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1827\u001b[1;33m             \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1828\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m             \u001b[1;31m# The error raised by numpy has no shape information, we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_NN_models(feature, feature_range):\n",
    "    feature_data = []\n",
    "    for r in feature_range:\n",
    "        if r != 'batch_size':\n",
    "            model = create_model(**{feature: r, 'num_hidden_nodes': 100})\n",
    "            model.fit(x_train, y_train_binary, verbose = False, epochs = 100)\n",
    "        else:\n",
    "            model = create_model(**{'num_hidden_nodes': 100})\n",
    "            model.fit(x_train, y_train_binary, verbose = False, batch_size = r, epochs = 100)\n",
    "        loss, f1 = model.evaluate(x_test, y_test_binary, verbose=0)\n",
    "        feature_data.append(f1)\n",
    "        print(r)\n",
    "    if type(feature_range[0]) == str:\n",
    "        plt.scatter(np.arange(0, len(feature_range)), feature_data)\n",
    "        plt.xticks(np.arange(0, len(feature_range)), feature_range)\n",
    "    else:\n",
    "        plt.scatter(feature_range, feature_data)\n",
    "    plt.title('Effect of Changing %s on F1-Score' % (feature))\n",
    "    plt.show()\n",
    "    \n",
    "#plot_NN_models('num_hidden_nodes', [5,10,20,50,100,200])\n",
    "#plot_NN_models('num_hidden_layers', [1,2,3,4,5])\n",
    "#plot_NN_models('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "plot_NN_models('batch_size', [5,15,25,50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.9245 - f1_m: 0.4738     \n",
      "Epoch 2/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.6907 - f1_m: 0.6930     \n",
      "Epoch 3/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.5941 - f1_m: 0.7559     \n",
      "Epoch 4/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.5237 - f1_m: 0.7923     \n",
      "Epoch 5/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.4768 - f1_m: 0.8207     \n",
      "Epoch 6/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.4356 - f1_m: 0.8412     \n",
      "Epoch 7/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.4092 - f1_m: 0.8528     \n",
      "Epoch 8/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.3854 - f1_m: 0.8599     \n",
      "Epoch 9/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.3616 - f1_m: 0.8699     \n",
      "Epoch 10/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.3430 - f1_m: 0.8800     \n",
      "Epoch 11/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.3298 - f1_m: 0.8853     \n",
      "Epoch 12/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.3188 - f1_m: 0.8837     \n",
      "Epoch 13/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.3052 - f1_m: 0.8932     \n",
      "Epoch 14/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2963 - f1_m: 0.8963     \n",
      "Epoch 15/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2851 - f1_m: 0.8967     \n",
      "Epoch 16/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2827 - f1_m: 0.8978     \n",
      "Epoch 17/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2689 - f1_m: 0.9080     \n",
      "Epoch 18/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2634 - f1_m: 0.9033     \n",
      "Epoch 19/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2559 - f1_m: 0.9074     \n",
      "Epoch 20/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2511 - f1_m: 0.9082     \n",
      "Epoch 21/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2454 - f1_m: 0.9126     \n",
      "Epoch 22/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2392 - f1_m: 0.9088     \n",
      "Epoch 23/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2339 - f1_m: 0.9122     \n",
      "Epoch 24/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2284 - f1_m: 0.9143     \n",
      "Epoch 25/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2229 - f1_m: 0.9204     \n",
      "Epoch 26/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2205 - f1_m: 0.9185     \n",
      "Epoch 27/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2152 - f1_m: 0.9239     \n",
      "Epoch 28/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2056 - f1_m: 0.9256     \n",
      "Epoch 29/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2107 - f1_m: 0.9210     \n",
      "Epoch 30/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.2057 - f1_m: 0.9212     \n",
      "Epoch 31/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1995 - f1_m: 0.9254     \n",
      "Epoch 32/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1926 - f1_m: 0.9277     \n",
      "Epoch 33/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1920 - f1_m: 0.9289     \n",
      "Epoch 34/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1873 - f1_m: 0.9318     \n",
      "Epoch 35/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1860 - f1_m: 0.9314     \n",
      "Epoch 36/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1822 - f1_m: 0.9341     \n",
      "Epoch 37/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1773 - f1_m: 0.9355     \n",
      "Epoch 38/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1731 - f1_m: 0.9367     \n",
      "Epoch 39/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1774 - f1_m: 0.9345     \n",
      "Epoch 40/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1692 - f1_m: 0.9389     \n",
      "Epoch 41/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1701 - f1_m: 0.9384     \n",
      "Epoch 42/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1645 - f1_m: 0.9376     \n",
      "Epoch 43/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1636 - f1_m: 0.9403     \n",
      "Epoch 44/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1600 - f1_m: 0.9404     \n",
      "Epoch 45/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1542 - f1_m: 0.9412     \n",
      "Epoch 46/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1639 - f1_m: 0.9409     \n",
      "Epoch 47/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1497 - f1_m: 0.9414     \n",
      "Epoch 48/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1503 - f1_m: 0.9441     \n",
      "Epoch 49/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1461 - f1_m: 0.9468     \n",
      "Epoch 50/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1448 - f1_m: 0.9429     \n",
      "Epoch 51/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1462 - f1_m: 0.9471     \n",
      "Epoch 52/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1438 - f1_m: 0.9451     \n",
      "Epoch 53/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1356 - f1_m: 0.9489     \n",
      "Epoch 54/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1361 - f1_m: 0.9516     \n",
      "Epoch 55/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1364 - f1_m: 0.9506     \n",
      "Epoch 56/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1343 - f1_m: 0.9504     \n",
      "Epoch 57/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1294 - f1_m: 0.9539     \n",
      "Epoch 58/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1351 - f1_m: 0.9477     \n",
      "Epoch 59/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1285 - f1_m: 0.9511     \n",
      "Epoch 60/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1232 - f1_m: 0.9551     \n",
      "Epoch 61/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1299 - f1_m: 0.9512     \n",
      "Epoch 62/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1256 - f1_m: 0.9540     \n",
      "Epoch 63/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1281 - f1_m: 0.9495     \n",
      "Epoch 64/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1177 - f1_m: 0.9583     \n",
      "Epoch 65/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1185 - f1_m: 0.9569     \n",
      "Epoch 66/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1206 - f1_m: 0.9560     \n",
      "Epoch 67/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1159 - f1_m: 0.9561     \n",
      "Epoch 68/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1186 - f1_m: 0.9571     \n",
      "Epoch 69/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1141 - f1_m: 0.9556     \n",
      "Epoch 70/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1099 - f1_m: 0.9603     \n",
      "Epoch 71/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1128 - f1_m: 0.9616     \n",
      "Epoch 72/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1138 - f1_m: 0.9546     \n",
      "Epoch 73/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1032 - f1_m: 0.9629     \n",
      "Epoch 74/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1043 - f1_m: 0.9644     \n",
      "Epoch 75/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1079 - f1_m: 0.9612     \n",
      "Epoch 76/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1081 - f1_m: 0.9592     \n",
      "Epoch 77/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1018 - f1_m: 0.9626     \n",
      "Epoch 78/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1077 - f1_m: 0.9586     \n",
      "Epoch 79/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1002 - f1_m: 0.9654     \n",
      "Epoch 80/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0991 - f1_m: 0.9614     \n",
      "Epoch 81/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1049 - f1_m: 0.9600     \n",
      "Epoch 82/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0993 - f1_m: 0.9654     \n",
      "Epoch 83/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0969 - f1_m: 0.9655     \n",
      "Epoch 84/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.1013 - f1_m: 0.9633     \n",
      "Epoch 85/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0986 - f1_m: 0.9622     \n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4092/4092 [==============================] - 0s - loss: 0.0937 - f1_m: 0.9662     \n",
      "Epoch 87/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0980 - f1_m: 0.9648     \n",
      "Epoch 88/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0990 - f1_m: 0.9635     \n",
      "Epoch 89/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0949 - f1_m: 0.9661     \n",
      "Epoch 90/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0954 - f1_m: 0.9669     \n",
      "Epoch 91/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0955 - f1_m: 0.9667     \n",
      "Epoch 92/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0940 - f1_m: 0.9640     \n",
      "Epoch 93/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0881 - f1_m: 0.9670     \n",
      "Epoch 94/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0940 - f1_m: 0.9677     \n",
      "Epoch 95/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0860 - f1_m: 0.9688      ETA: 0s - loss: 0.0888 - f1_m: \n",
      "Epoch 96/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0881 - f1_m: 0.9693     \n",
      "Epoch 97/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0888 - f1_m: 0.9673     \n",
      "Epoch 98/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0902 - f1_m: 0.9658     \n",
      "Epoch 99/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0907 - f1_m: 0.9664     \n",
      "Epoch 100/100\n",
      "4092/4092 [==============================] - 0s - loss: 0.0839 - f1_m: 0.9714     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x300c35d0>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train_binary, batch_size = 15, epochs = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  32/1364 [..............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19684809476923734, 0.9362169490182155]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       525\n",
      "           1       0.97      0.96      0.96       534\n",
      "           2       0.93      0.96      0.94        78\n",
      "           3       0.94      0.85      0.89       227\n",
      "\n",
      "    accuracy                           0.94      1364\n",
      "   macro avg       0.94      0.93      0.93      1364\n",
      "weighted avg       0.94      0.94      0.94      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(model.predict(x_test), axis = 1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10) # Define the split - into 5 folds \n",
    "kf.get_n_splits(x_test) # returns the number of splitting iterations in the cross-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_train_avgs = []\n",
    "forest_test_avgs = []\n",
    "\n",
    "for train_index, test_index in kf.split(x_test):\n",
    "    X_train, X_test = x_test[train_index], x_test[test_index]\n",
    "    Y_train, Y_test = y_test[train_index], y_test[test_index]\n",
    "    forest_train_avgs.append(f1_score(Y_train, best_forest_estimator.predict(X_train), average='weighted'))\n",
    "    forest_test_avgs.append(f1_score(Y_test, best_forest_estimator.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9991843282386351,\n",
       "  0.9991843555819035,\n",
       "  0.9991843423334152,\n",
       "  1.0,\n",
       "  0.9991849924664538,\n",
       "  0.9991849840062541,\n",
       "  0.9991849427394225,\n",
       "  0.9991849998442911,\n",
       "  0.9991849653624052,\n",
       "  0.999184995730574],\n",
       " [1.0, 1.0, 1.0, 0.9926534797970695, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_train_avgs, forest_test_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_train_avgs = []\n",
    "svm_test_avgs = []\n",
    "\n",
    "for train_index, test_index in kf.split(x_test):\n",
    "    X_train, X_test = x_test[train_index], x_test[test_index]\n",
    "    Y_train, Y_test = y_test[train_index], y_test[test_index]\n",
    "    svm_train_avgs.append(f1_score(Y_train, best_svm_estimator.predict(X_train), average='weighted'))\n",
    "    svm_test_avgs.append(f1_score(Y_test, best_svm_estimator.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9309346933202884,\n",
       "  0.9301240911757788,\n",
       "  0.9317137557426819,\n",
       "  0.9317234221233436,\n",
       "  0.9309584883283203,\n",
       "  0.9317140514485268,\n",
       "  0.929316269028127,\n",
       "  0.9293433345317933,\n",
       "  0.9293355234192419,\n",
       "  0.9301434182722756],\n",
       " [0.9270443794956316,\n",
       "  0.9343139986255827,\n",
       "  0.9196395132877881,\n",
       "  0.9190608398193928,\n",
       "  0.925670102894186,\n",
       "  0.9200658270727476,\n",
       "  0.9415545468948361,\n",
       "  0.9411043353260817,\n",
       "  0.9422093719404091,\n",
       "  0.9335546962147692])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train_avgs, svm_test_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_train_avgs = []\n",
    "nn_test_avgs = []\n",
    "\n",
    "for train_index, test_index in kf.split(x_test):\n",
    "    X_train, X_test = x_test[train_index], x_test[test_index]\n",
    "    Y_train, Y_test = y_test[train_index], y_test[test_index]\n",
    "    nn_train_avgs.append(f1_score(Y_train, np.argmax(model.predict(X_train), axis =1 ), average='weighted'))\n",
    "    nn_test_avgs.append(f1_score(Y_test, np.argmax(model.predict(X_test), axis = 1), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9469707777439208,\n",
       "  0.9445697136055974,\n",
       "  0.9453384731348627,\n",
       "  0.9486363496159125,\n",
       "  0.9469956797818884,\n",
       "  0.9462061065969861,\n",
       "  0.9453547695232936,\n",
       "  0.945384127842897,\n",
       "  0.9479020405040651,\n",
       "  0.9470127251405714],\n",
       " [0.9421683519721423,\n",
       "  0.9635066847647114,\n",
       "  0.9562542350863518,\n",
       "  0.9265524222427636,\n",
       "  0.941917852335767,\n",
       "  0.9485294117647058,\n",
       "  0.9559990662931839,\n",
       "  0.9559307912888474,\n",
       "  0.9339565197721872,\n",
       "  0.9410930470325103])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_train_avgs, nn_test_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99926629 0.9305307  0.94643708]\n",
      "[0.00024457 0.00095902 0.00122223]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([forest_train_avgs, svm_train_avgs, nn_train_avgs], axis = 1))\n",
    "print(np.std([forest_train_avgs, svm_train_avgs, nn_train_avgs], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99926535 0.93042176 0.94659084]\n",
      "[0.00220396 0.0088799  0.01091705]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([forest_test_avgs, svm_test_avgs, nn_test_avgs], axis = 1))\n",
    "print(np.std([forest_test_avgs, svm_test_avgs, nn_test_avgs], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pylab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement pylab (from versions: none)\n",
      "ERROR: No matching distribution found for pylab\n",
      "WARNING: You are using pip version 19.2.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2f9088d0>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAFCCAYAAAC90NpzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElShQxjkiRJFTKMSZIkVej/AeVGM1w7DyZAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f735270>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "plt.scatter(np.arange(0, len(forest_test_avgs)), forest_test_avgs, c = 'r', label = 'test')\n",
    "plt.scatter(np.arange(0, len(forest_train_avgs)), forest_train_avgs, c = 'b', label = 'train')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Evaluation Data for Tuned Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2ffb0a10>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAFCCAYAAADL6mj4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVcDQJUmSVIH/D/4aEsdt73dIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2fdd00b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "plt.scatter(np.arange(0, len(svm_test_avgs)), svm_test_avgs, c = 'r', label = 'test')\n",
    "plt.scatter(np.arange(0, len(svm_train_avgs)), svm_train_avgs, c = 'b', label = 'train')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Evaluation Data for Tuned SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2ffee770>"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAFCCAYAAADL6mj4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJTB0SZIklcDQJUmSVAJDlyRJUgn+f+QNyzOVHgMzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2fe0d5f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "plt.scatter(np.arange(0, len(nn_test_avgs)), nn_test_avgs, c = 'r', label = 'test')\n",
    "plt.scatter(np.arange(0, len(nn_train_avgs)), nn_train_avgs, c = 'b', label = 'train')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Evaluation Data for Tuned NN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
